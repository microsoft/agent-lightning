# Fine-tune with Unsloth SFT

!!! note "Prerequisites"

    Please make sure you have read [Write the First Algorithm](./write-first-algorithm.md). Although that recipe is based on a simple prompt tuning algorithm, it introduces the core concepts of Agent-lightning and you should be familiar with them before proceeding.

This recipe builds on [Write the First Algorithm](./write-first-algorithm.md). Instead of iterating on a prompt, we will iterate on a large language model with [Unsloth](https://docs.unsloth.ai/)'s SFT Trainer and keep the whole loop inside Agent-lightning. The new pieces you will meet are the **LLM proxy**, the **trace-to-triplet adapter**, a [vLLM](https://github.com/vllm-project/vllm) inference endpoint, and an agent implemented with the [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/).

!!! warning

    You need a GPU that can host the Unsloth base model and run vLLM. The sample defaults to the `unsloth/Qwen3-4B-Instruct-2507`, which will require at least 16GB of GPU memory under 4-bit quantization.

## The Data and Serving Loop

To tune a large language model in Supervised Fine-Tuning (SFT), we commonly need a dataset with input/output samples. For example, the [TRL SFT Trainer](https://huggingface.co/docs/trl/sft_trainer) expects a dataset with samples like the following:

```json
{"messages": [{"role": "user", "content": "What color is the sky?"},
              {"role": "assistant", "content": "It is blue."}]}
```

With supervised fine-tuning, the LLM learns to generate the "assistant" response as close as possible to the completion in the dataset.

Typically, the dataset used in SFT should be a curated set of samples. The samples can be either hand-written by humans, or generated by a more powerful model, which is known as [data distillation](https://docs.nvidia.com/nemo-framework/user-guide/24.12/modelalignment/knowledge-distillation.html). However, in this recipe, we invent a different setup, which uses the samples generated by the model itself to tune the model. We use reward emitted by the agent to select the top-performing samples.

Overall, the flow of the algorithm is an iteration of the following steps:

1. Serve the current checkpoint (with vLLM).
2. Publish the vLLM endpoint via LLM proxy and let runners rollout some tasks with the current model.
3. Collect the traces from the rollouts and transform the highest-rewarded ones into a dataset that is acceptable by Unsloth SFT Trainer.
4. Launch Unsloth to fine-tune on the dataset and save a new checkpoint.

You will find the full source code of this iteration in `sft_one_iter` in [sft_algorithm.py]({{ src("examples/unsloth/sft_algorithm.py") }}). We will elaborate more details on each part.

### Serving the Model with vLLM and Proxy

Most modern agents do not use the model directly, instead, they use an API like OpenAI chat completion API to interact with the model. Therefore, we need a vLLM-based inference server launched before rollouts. The serving code briefly look like the following. See `vllm_server` function in [sft_algorithm.py]({{ src("examples/unsloth/sft_algorithm.py") }}) if you want to see a more robust version.

```python
from contextlib import contextmanager
from openai import OpenAI

@contextmanager
def vllm_server(model_path: str, port: int):
    try:
        proc = subprocess.Popen([
            "vllm", "serve", model_path, "--port", str(port),
            "--enable-auto-tool-choice", "--tool-call-parser", "hermes"
        ])

        # Wait for the server to be ready
        url = f"http://localhost:{port}/health"
        start = time.time()
        client = httpx.Client()

        while True:
            if client.get(url).status_code == 200:
                break

        yield f"http://localhost:{port}/v1"
    finally:
        # Terminate the server
        proc.terminate()

# Use the vLLM server
with vllm_server(model_path, port) as server_address:
    # Use the server address to interact with the model
    openai = OpenAI(base_url=server_address)
    ...
```

In this recipe, we do not expose the server address directly to the agent runners, because we want to install a "middleware" to collect the prompts and responses of all the requests. In general cases, it's up to you to decide whether to hide the vLLM server behind a proxy or not.

The "middleware" here is [`LLMProxy`][agentlightning.LLMProxy], which is an independant [LiteLLM](https://docs.litellm.ai/) server that forwards the requests to the vLLM server. It also exposes an OpenAI-compatible API that the runners can target without caring about where the model lives. The benefits of using the proxy are:

1. **Traces:** The proxy automatically logs the prompts and responses of all the requests into the store.
2. **Token IDs:** The proxy augments the requests so that the vLLM server can return the prompt and response token IDs (see more details in [Serving LLM](../deep-dive/serving-llm.md)).

The [`LLMProxy`][agentlightning.LLMProxy] accepts a list of model configurations, in the same syntax as LiteLLM's [`model_list`](https://docs.litellm.ai/docs/proxy/configs). Include a `hosted_vllm/` prefix to the models to activate LiteLLM's [vLLM integration](https://docs.litellm.ai/docs/providers/vllm).

```python
llm_proxy = agl.LLMProxy(port=port, store=store)
model_list = [
    {
        "model_name": "Qwen3-4B-Instruct",
        "litellm_params": {"model": f"hosted_vllm/{model_path}", "api_base": server_address},
    }
]
llm_proxy.update_model_list(model_list)
# If the proxy is not running, it will start automatically.
llm_proxy.restart()
# Add the proxy as a resource to the store so that the runners can access it via URL.
store.add_resources({"main_llm": llm_proxy.as_resource()})
```

### Spawn Rollout and Collect Spans

TBD

### Adapt the Spans to HuggingFace Dataset

TBD

### Launch Unsloth Training

TBD

## The Agent: OpenAI Agents SDK with Tooling

`math_agent` is wrapped by the `@rollout` decorator so runners can execute it. It uses the OpenAI Agents SDK (`agents.Agent`, `Runner`) to wire a calculator MCP server and an OpenAI-compatible chat completion model together. The runner injects the `LLM` resource supplied by the proxy, so the agent can stay focused on the task logic: formatting requests with strict instructions (`### <answer> ###`) and computing a reward by comparing the extracted numeric answer to the GSM-hard ground truth.

TBD: make it more verbose. Include some code snippets.

## Run this Recipe

The full runnable script for this recipe resides in [`examples/unsloth`]({{ src("examples/unsloth") }}) folder.

Before running this example, install `unsloth`, `vllm`, and the other libraries used in the examples (the project uses CUDA tooling, TRL, rich, datasets, etc.). We tested with `unsloth==2025.10.1`. `unsloth==2025.10.2` and `2025.10.3` are not working because of an [issue](https://github.com/unslothai/unsloth/issues/3451) we have been investigating with the unsloth team.

It's recommended to download the base model before running the example, such that the first iteration and subsequent iterations can both load from local checkpoints.

```bash
hf download unsloth/Qwen3-4B-Instruct-2507 --local-dir models/version_0
```

The repository already contains `examples/unsloth/data_gsmhard.jsonl` (which is a very small subset of the [GSM-hard math dataset](https://huggingface.co/datasets/reasoning-machines/gsm-hard) for demonstration purposes).

### Run Manually

Similar to the [Write the First Algorithm](./write-first-algorithm.md) recipe, you can open three terminals and start each component in parallel.

```bash
agl store --port 4747
python examples/unsloth/sft_rollout_runners.py
python examples/unsloth/sft_algorithm.py
```

In this case, [`sft_rollout_runners.py`]({{ src("examples/unsloth/sft_rollout_runners.py") }}) is a simple spawner implemented in Python that spawns 4 runners in parallel. The runners all connect to the same store server executing in another terminal.

```python
import agentlightning as agl

def run_rollout(store: agl.LightningStore, worker_id: int) -> None:
    # Since the server side has already used LiteLLM proxy to collect traces,
    # a simple OtelTracer to collect the rewards is enough.
    tracer = agl.OtelTracer()

    runner = agl.LitAgentRunner(tracer=tracer)

    with runner.run_context(agent=math_agent, store=store, worker_id=worker_id):
        asyncio.run(runner.iter())


def spawn_runners(store: agl.LightningStore, n_runners: int) -> None:
    runners = [
        multiprocessing.Process(target=run_rollout, args=(store, worker_id))
        for worker_id in range(n_runners)
    ]
    for runner in runners:
        runner.start()

    for runner in runners:
        runner.join()


store = agl.LightningStoreClient("http://localhost:4747")
spawn_runners(store=store, n_runners=4)
```

!!! tip

    Try to swap [`OtelTracer`][agentlightning.OtelTracer] in the runners with other tracers like [`AgentOpsTracer`][agentlightning.AgentOpsTracer]. Try to use a different adapter at the algorithm side such as [`TracerTraceToTriplet`][agentlightning.TracerTraceToTriplet] to see what happens.

### Run Everything with Trainer

We also show how to wrap everything into a single script using [`Trainer`][agentlightning.Trainer]. [`sft_allinone.py`]({{ src("examples/unsloth/sft_allinone.py") }}) shows the full example code.

TBD: more details and how to run.
