# Serving LLMs under Agent-lightning

*Agent-lightning* focuses on data, learning signals, and control flow — **not** on running model inference. This deep dive explains how to **serve** a model beside Agent-lightning so runners can call it reliably, how the **LLM Proxy** plugs into the loop, and why **token IDs** matter if you care about correctness in training and evaluation.

## General background on LLM serving

Serving a model is a mandatory step if you want to train a model, especially when you want to use the model generated by the model itself as the training data. We quickly go through the general background to make sure all readers are on the same page.

Modern LLM servers solve a hard scheduling problem: keep GPUs hot while juggling prompts of different lengths, streaming tokens as they arrive, and squeezing large KV caches into limited memory. Techniques like [**continuous batching**](https://www.anyscale.com/blog/continuous-batching-llm-inference) and [**paged attention**](https://arxiv.org/abs/2309.06180) emerged to address this. Continuous batching interleaves decoding across requests to reuse weights efficiently; with good memory planning it delivers big throughput wins at the same latency budget. PagedAttention reduces KV-cache fragmentation so batching remains effective even as sequences grow. See overviews and results in [vLLM’s PagedAttention paper](https://arxiv.org/abs/2309.06180) and [industry write-ups](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/). The correctness and efficiency of inference are both big problems and really difficult to balance. A [recent blog](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) from Thinking Machines Labs highlight the nondeterminism nature and how it finally affects the training.

On top of scheduling, servers expose an HTTP API, often **OpenAI-compatible** (`/v1/chat/completions` and `/v1/responses`), which is another set of heavy stack to maintain. Other than text prompts and chat messages, the API also defines dozens of other parameters and response fields including [tool calls](https://platform.openai.com/docs/guides/function-calling), [structured output](https://platform.openai.com/docs/guides/structured-outputs) and [multi-modality support](https://platform.openai.com/docs/guides/images-vision). Popular engines like **vLLM** and [**SGLang**](https://github.com/sgl-project/sglang) all ship an OpenAI-compatible frontend so you can reuse existing client code. [Ollama](https://ollama.com/blog/openai-compatibility) and [llama.cpp](https://llama-cpp-python.readthedocs.io/en/latest/server/) also provide similar capability. However, due to the non-standardized nature of the model themselves, each framework has different interpretations and implementations of the API. Even if you pass the same request, if processed by different frameworks, the tokens fed into the underlying model might be largely different.

Nevertheless, most of the problems listed above either have mitigations or unsolved yet. Readers only need to have them in mind. The real problem we care about is: what does Agent-lightning expect from a served LLM? I think the answer is at least two things:

* An OpenAI-compatible **Chat Completions** / **Responses** path the agent can hit during rollouts.
* Optional signals useful for training and debugging: **logprobs**, **usage**, and, ideally, **token IDs**. (OpenAI’s public Chat Completions spec exposes usage and logprobs, but **not** token IDs; more on why IDs matter later.)

## Launching a serving framework

For many algorithms, you’ll start an engine (e.g., **vLLM** or **SGLang**) right before rollouts, then take it down between iterations to free GPU memory. Many LLM serving frameworks all provide a one-line “serve” entry point and OpenAI-compatible docs. Use those to stand up `/v1/chat/completions` with your checkpoint; make sure streaming and any tool-calling that your agent might use are enabled. A recipe that shows how to do this is [Unsloth SFT](../how-to/unsloth-sft.md).

Weight updates, which happens after one training step is completed, however, can be more tricky. Some frameworks like [vLLM](https://vllm.ai/) support hot-updating the model weights of the inference engine. However, it's generally favored and more straightforward to just restart the engine to load the new weights. For medium-sized tasks with a batch of hundreds of rollouts, the overhead of restarting the engine is usually negligible.

If you’re using Agent-lightning’s [**VERL**][agentlightning.algorithm.verl.VERL] integration, the algorithm can **manage the server for you**. [VERL framework](https://verl.readthedocs.io/en/v0.5.x/advance/agent_loop.html) is really clever in managing all the computation resources and it wraps vLLM/SGLang behind an `AsyncLLMServer` abstraction. You can directly use that server as a LLM endpoint for the agents, but because VERL can create multiple replicas of the vLLM server, it might be safer to let [`LLMProxy`][agentlightning.LLMProxy] guard them for you.

## LLM Proxy

The **LLM Proxy** is a utility class provided by Agent-lightning that is built upon [LiteLLM](https://docs.litellm.ai/) and sits between runners and your backend engine(s) / server(s). In Agent-lightning it acts as a single URL you register as a [`Resource`][agentlightning.Resource] in the store, and it provides three kinds of value:

1. **Unified endpoint & hot-swaps.** You can move traffic between OpenAI, Anthropic, local vLLM/SGLang, or canary checkpoints without touching agent code — just point the proxy at a different backend.
2. **First-class tracing.** The proxy emits **OpenTelemetry** spans for every call and ships them to the [`LightningStore`][agentlightning.LightningStore]. The proxy carries the rollout and attempt identifiers in the request headers, so that the spans can be properly attributed to the correct rollout and attempt. The proxy also allocates a monotonic sequence number per span (via the store) to avoid clock-skew issues and lets adapters reconstruct execution trees reliably.
3. **Token IDs.** The proxy can return the prompt and response token IDs along with the response. See below for more details.

Operationally, keeping the proxy next to the algorithm process works well: the algorithm registers the backend (e.g., the vLLM URL) to the proxy (via [`LLMProxy.update_model_list`][agentlightning.LLMProxy.update_model_list]), publishes the proxy URL to the store as a resource (via [`LightningStore.add_resources`][agentlightning.LightningStore.add_resources]), and runners simply use that URL during rollouts. This is the same topology you’ll see in many production client-server strategies.

## Token IDs and why they matter

We will give a deep dive about how Agent-lightning handles and uses Token IDs. It's a subtle point that is often overlooked in the community, but we show it might make a big difference in the training stability and accuracy.

Most agents in the wild call **Chat Completions** and use chat messages to communicate with LLMs. There are two mainstream approaches in the community to collect data from these agents.

**The first approach** is to keep the chat completion messages as the saved data format, and let training algorithms **retokenize** the messages into token IDs. This is the approach taken by many SFT practices such as [HuggingFace SFT](https://huggingface.co/docs/trl/sft_trainer).

In practice, we found this method is flawed and lead to poor training stability and accuracy. We show a comparison of the training results in the following chart.

<div style="height:400px">
<canvas data-chart='{
  "type": "line",
  "data": {
    "labels": [0.0, 32.0, 64.0, 96.0, 128.0, 160.0, 192.0, 224.0, 256.0, 288.0, 320.0, 352.0, 384.0, 416.0, 448.0, 480.0],
    "datasets": [
      {
        "label": "With Token IDs from Retokenization",
        "data": [0.49, 0.512, 0.54, 0.532, 0.54, 0.466, 0.328, 0.358, 0.348, 0.35, 0.346, 0.372, 0.346, 0.33, 0.346, 0.332],
        "spanGaps": true
      },
      {
        "label": "Retokenization (Run Again)",
        "data": [0.494, 0.526, 0.536, 0.554, 0.544, 0.556, 0.568, 0.552, 0.45, 0.466, 0.474, 0.47, 0.464, 0.476, 0.488, 0.432],
        "spanGaps": true
      },
      {
        "label": "With Token IDs from Engine",
        "data": [0.494, 0.522, 0.514, 0.538, 0.53, 0.564, 0.564, 0.586, 0.594, 0.604, 0.618, 0.584, 0.606, 0.558, 0.612, 0.588],
        "spanGaps": true
      }
    ]
  },
  "options": {
    "interaction": {
      "mode": "nearest",
      "intersect": false
    },
    "plugins": {
      "legend": {
        "display": true,
        "position": "top"
      },
      "title": {
        "display": true,
        "text": "Agent Training Results Comparison"
      }
    },
    "scales": {
      "x": {
        "title": {
          "display": true,
          "text": "Step"
        }
      },
      "y": {
        "title": {
          "display": true,
          "text": "Reward"
        }
      }
    }
  }
}'></canvas>
</div>

The reason behind this is in two folds. On one hand, a word might be produced during generation as two tokens (e.g., `H + AVING`), but when you re‑tokenize the text later you get a different split (e.g., `HAV + ING`). The text looks identical, but the IDs might be different from what the model is actually more comfortable with. On the other hand, subtle differences might appear when you try to retokenize a tool call. When a tool call (e.g., `<tool_call>{ "name": ... }</tool_call>`) is parsed, a lot of information such as space characters and formatting might be lost. The tool call parser might also tries to fix some common JSON issues to make the tool call appearing to be correct. But these fixes hide the real problem that the original generated tokens are faulty in the first place, and these faults will never get a chance to be trained and corrected.

**The second approach** is to save the Token IDs directly to the training data. This is the approach taken by many RL practices such as [Tinker](https://thinkingmachines.ai/tinker/). A large problem with this approach is that you end up needing to build a training framework with "tokens" as the first-class citizen. That is to say, the agent has to interact with the inference engine with tokens, so that you can be able to capture them for training.

This is a problem for many agents, because many agents, especially those using existing agent frameworks like LangChain, are designed to work with OpenAI-compatible APIs. They do not have the capability to tokenize and detokenize the messages themselves. As discussed [above](#general-background-on-llm-serving), this can be a very heavy layer and it can be very error-prone if you implement it yourself. So many solutions either implement the solution on their own (e.g., [VERL Agent Loop](https://github.com/volcengine/verl/blob/4da0d3d3188072772cb2ec817b3d6cf4a463821f/recipe/langgraph_agent/chat_model.py), [Tinker Renderer](https://github.com/thinking-machines-lab/tinker-cookbook/blob/34a6588d7055040c259985d98e71c0140b389ba7/tinker_cookbook/renderers.py)), or simply give up and ask users to implement it by themselves (e.g., [SkyRL Search-R1](https://novasky-ai.notion.site/skyrl-searchr1)).

We believe a better solution could be **to have an OpenAI-compatible API that can return token IDs directly.** This way, the agent can still work in the same old way with the OpenAI-compatible APIs, but you can also be able to capture the token IDs via [tracing](../tutorials/traces.md) and train on top of those token IDs directly.

When Agent-lightning is first released, we implemented an instrumented vLLM server, which monkey-patches the vLLM OpenAI server implementation and return the token IDs along with the response. The good news is that this capability has been merged into [vLLM upstream](https://github.com/vllm-project/vllm/pull/22587). Starting from vLLM 0.10.2, it has a `return_token_ids` parameter so you can request for token IDs alongside the chat messages; SGLang has tracked [similar features and flags](https://github.com/sgl-project/sglang/issues/2634) around exposing `prompt_token_ids`/`token_ids`. But it's not supported in their OpenAI-compatible layer at the moment this document is written.

Putting it simply, working with vLLM v0.10.2 or later, our [`LLMProxy`][agentlightning.LLMProxy] will augment the request and add an extra parameter `return_token_ids` to the request. The engine will then return the token IDs along with the response. For earlier versions of vLLM, the augmented version of vLLM is still needed.

Theoretically, saving the token IDs into spans will also have certain limitations. For example, if you are using spans collected from one model to train another model, you will be in trouble if two models use different tokenizers. However in practice, our spans always strive to save both text and token IDs. Therefore, you can always use the retokenization approach as a fallback.

---

## Putting it together under Agent-lightning

A typical flow for serving under Agent-lightning looks like this:

1. **Start an engine.** Launch vLLM or SGLang with your checkpoint, context length, and features you need (streaming, tool calling, logprobs). vLLM’s docs cover the OpenAI-compatible server and common flags. ([docs.vllm.ai][2])

2. **Front it with the LLM Proxy.** Register the backend with the proxy; publish the proxy URL as a **resource** in the LightningStore. Runners fetch resources with rollouts and call the proxy instead of the engine directly. The proxy exports OpenTelemetry spans to the store so your traces include prompts, responses, usage, and, when supported by the backend, **token IDs**. ([docs.litellm.ai][5])

3. **Roll out and collect.** Runners stream tokens from the proxy; the proxy and runner side both emit spans. Deterministic span ordering comes from the store’s sequence allocator, which the proxy consults before forwarding.

4. **Adapt for training.** On the algorithm side, query spans and run an adapter (e.g., trace → triplets). If token IDs are present, build datasets directly with `input_ids/labels` instead of re-tokenizing; this keeps the dataset faithful to what the model truly saw.

---

## Notes on performance and operability

Throughput is dominated by how well your engine batches and manages memory; PagedAttention and continuous batching are the main levers. At deployment time, confirm the **chat template** the engine uses (vLLM relies on the tokenizer’s `apply_chat_template`), because template changes can introduce subtle tokenization drift. Track template/tokenizer versions in span attributes for auditability. ([GitHub][8])

If you need richer evaluation signals, enable **logprobs** on Chat Completions (OpenAI-compatible shape is supported by vLLM with some caveats that evolved over time). When you need **token IDs**, use the engine’s `return_token_ids` (vLLM) or the equivalent facility; vLLM release notes call out the parameter explicitly. ([GitHub][9])

Finally, remember that some RL libraries and agent loops (e.g., in VERL) still prefer the **IDs-level** interface for end-to-end control. That’s a fine choice when you own the loop; the Agent-lightning story here is to make the IDs available **even when** the agent uses standard Chat Completions. ([verl.readthedocs.io][4])

---

## Quick link board (for deeper reading)

* vLLM OpenAI-compatible server docs — how to serve chat completions locally or on Ray/other backends. ([docs.vllm.ai][2])
* Continuous batching & PagedAttention — why serving is hard and how modern engines fix it. ([Anyscale][10])
* LiteLLM + OpenTelemetry — tracing hooks you’ll benefit from when using a proxy. ([docs.litellm.ai][5])
* VERL agent loop — chat vs token-in/token-out APIs; why some RL stacks avoid Chat Completions. ([verl.readthedocs.io][4])
* vLLM `return_token_ids` — release notes mentioning the parameter to fetch IDs via chat completions. ([GitHub][6])

---

### One mental model to carry forward

Agent-lightning does not *serve* your LLM; it organizes **who talks to whom** and **what data is captured**. Put a high-quality engine behind the **LLM Proxy**, ask that engine to **return token IDs** with responses, and let the store’s traces be the single source of truth. That combination preserves fidelity for training and keeps your serving choices flexible as your system evolves.

[1]: https://arxiv.org/abs/2309.06180?utm_source=chatgpt.com "Efficient Memory Management for Large Language Model Serving with PagedAttention"
[2]: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html?utm_source=chatgpt.com "OpenAI-Compatible Server - vLLM"
[3]: https://platform.openai.com/docs/api-reference/chat?utm_source=chatgpt.com "OpenAI's Chat Completion API Documentation"
[4]: https://verl.readthedocs.io/en/v0.5.x/advance/agent_loop.html?utm_source=chatgpt.com "Agent Loop - verl documentation - Read the Docs"
[5]: https://docs.litellm.ai/docs/observability/opentelemetry_integration?utm_source=chatgpt.com "OpenTelemetry - Tracing LLMs with any observability tool"
[6]: https://github.com/vllm-project/vllm/releases?utm_source=chatgpt.com "Releases · vllm-project/vllm"
[7]: https://cookbook.openai.com/examples/using_logprobs?utm_source=chatgpt.com "Using logprobs"
[8]: https://github.com/vllm-project/vllm/issues/2012?utm_source=chatgpt.com "\"/v1/chat/completions\" tokenization issue #2012"
[9]: https://github.com/vllm-project/vllm/issues/3179?utm_source=chatgpt.com "The logprobs in the ChatCompletion's responses is ..."
[10]: https://www.anyscale.com/blog/continuous-batching-llm-inference?utm_source=chatgpt.com "Achieve 23x LLM Inference Throughput & Reduce p50 ..."
